# Config file for training script
dataset_type: "MistralCRD3Dataset"
lora_args:
  r: 4
  lora_alpha: 4
  lora_dropout: 0.1
train_args:
  eval_steps: 16
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 16
  logging_dir: "crd3_unsloth_mistral_lora_logs"
  logging_steps: 1
  max_steps: 8192
  output_dir: "crd3_unsloth_mistral_lora"
  save_steps: 32
resume_from_checkpoint: "crd3_unsloth_mistral_lora/checkpoint-192"
