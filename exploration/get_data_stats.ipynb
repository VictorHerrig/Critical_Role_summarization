{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_path = '../src/loaders'\n",
    "sys.path.append(dataset_path)\n",
    "from CRD3Dataset import CRD3Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg_path = '../src/loaders/CRD3Dataset_all.yaml'\n",
    "dataset = CRD3Dataset(cfg_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsing_tokenizer = dataset.tokenizer\n",
    "n_chunks = 0\n",
    "min_utt = ''\n",
    "max_utt = ''\n",
    "utt_lens = []\n",
    "min_summary = ''\n",
    "max_summary = ''\n",
    "summ_lens = []\n",
    "turns_per_sum = []\n",
    "current_fn = ''\n",
    "chunks_per_file = []\n",
    "turns_per_file = []\n",
    "tokens_per_file = []\n",
    "\n",
    "for fn, _, utts, summ in dataset.iter_chunk_w_filename():\n",
    "    if len(summ) < 1:\n",
    "        continue\n",
    "    if fn != current_fn:\n",
    "        current_fn = fn\n",
    "        chunks_per_file.append(0)\n",
    "        turns_per_file.append(0)\n",
    "        tokens_per_file.append(0)\n",
    "    n_chunks += 1\n",
    "    chunks_per_file[-1] += 1\n",
    "    turns_per_file[-1] += len(utts)\n",
    "    turns_per_sum.append(len(utts))\n",
    "    n_utt_tokens = len(parsing_tokenizer.encode((' '.join(utts))).ids)\n",
    "    if len(utt_lens) < 1 or n_utt_tokens < min(utt_lens):\n",
    "        min_utt = ' '.join(utts)\n",
    "    elif n_utt_tokens > max(utt_lens):\n",
    "        max_utt = ' '.join(utts)\n",
    "    utt_lens.append(n_utt_tokens)\n",
    "    tokens_per_file[-1] += n_utt_tokens\n",
    "\n",
    "    n_summ_tokens = len(parsing_tokenizer.encode(summ).ids)\n",
    "    if len(summ_lens) < 1 or  n_summ_tokens < min(summ_lens):\n",
    "        min_summary = summ\n",
    "    elif n_summ_tokens > max(summ_lens):\n",
    "        max_summary = summ\n",
    "    summ_lens.append(n_summ_tokens)\n",
    "\n",
    "print('Chunks:', n_chunks)\n",
    "print('Min utterances token length:', min(utt_lens))\n",
    "print('Min utterances:')\n",
    "print(min_utt)\n",
    "print('Max utterances token length:', max(utt_lens))\n",
    "print('Max utterances clip:')\n",
    "print(max_utt[:200] + '...')\n",
    "print('Min summary token length:', min(summ_lens))\n",
    "print('Min summary:')\n",
    "print(min_summary)\n",
    "print('Max summary token length:', max(summ_lens))\n",
    "print('Max summary clip:')\n",
    "print(max_summary[:200] + '...')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "_ = plt.subplots(figsize=(10, 6))\n",
    "bins = np.logspace(np.log10(min(utt_lens)), np.log10(max(utt_lens)))\n",
    "plt.hist(utt_lens, label='Utt', color='dodgerblue', bins=bins)\n",
    "plt.hist(summ_lens, label='Summary', color='darkorange', bins=bins)\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.title('Summary and document tokens')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "_ = plt.subplots(figsize=(10, 6))\n",
    "ratios = [float(u) / float(s) for u, s in zip(utt_lens, summ_lens) if s != 0]\n",
    "bins = np.logspace(np.log10(min(ratios)), np.log10(max(ratios)))\n",
    "plt.hist(ratios, color='forestgreen', bins=bins)\n",
    "plt.xscale('log')\n",
    "plt.title('Document / Summary compression ratio')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.percentile(utt_lens, 94), np.percentile(summ_lens, 94)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(utt_lens), len(summ_lens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "upper_10_idx = np.argsort(utt_lens)[-int(len(utt_lens) / 10):]\n",
    "upper_10_utt_lens = np.array(utt_lens)[upper_10_idx]\n",
    "upper_10_summ_lens = np.array(summ_lens)[upper_10_idx]\n",
    "\n",
    "plt.clf()\n",
    "_ = plt.subplots(figsize=(10, 6))\n",
    "bins = np.logspace(np.log10(min(upper_10_utt_lens.min(), upper_10_summ_lens.min())), np.log10(max(upper_10_utt_lens)))\n",
    "plt.hist(upper_10_utt_lens, label='Utt', color='dodgerblue', bins=bins)\n",
    "plt.hist(upper_10_summ_lens, label='Summary', color='darkorange', bins=bins)\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.title('Upper 10% document length summary and document tokens')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "_ = plt.subplots(figsize=(10, 6))\n",
    "ratios = [float(u) / float(s) for u, s in zip(upper_10_utt_lens, upper_10_summ_lens) if s != 0]\n",
    "bins = np.logspace(np.log10(min(ratios)), np.log10(max(ratios)))\n",
    "plt.hist(ratios, color='forestgreen', bins=bins)\n",
    "plt.xscale('log')\n",
    "plt.title('Upper 10% document length compression ratio')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lower_90_idx = np.argsort(utt_lens)[:-int(len(utt_lens) / 10)]\n",
    "lower_90_utt_lens = np.array(utt_lens)[lower_90_idx]\n",
    "lower_90_summ_lens = np.array(summ_lens)[lower_90_idx]\n",
    "\n",
    "plt.clf()\n",
    "_ = plt.subplots(figsize=(10, 6))\n",
    "bins = np.logspace(np.log10(min(lower_90_utt_lens.min(), lower_90_summ_lens.min())), np.log10(max(lower_90_utt_lens)))\n",
    "plt.hist(lower_90_utt_lens, label='Utt', color='dodgerblue', bins=bins)\n",
    "plt.hist(lower_90_summ_lens, label='Summary', color='darkorange', bins=bins)\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.title('Lower 90% document length summary and document tokens')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "_ = plt.subplots(figsize=(10, 6))\n",
    "ratios = [float(u) / float(s) for u, s in zip(lower_90_utt_lens, lower_90_summ_lens) if s != 0]\n",
    "bins = np.logspace(np.log10(min(ratios)), np.log10(max(ratios)))\n",
    "plt.hist(ratios, color='forestgreen', bins=bins)\n",
    "plt.xscale('log')\n",
    "plt.title('Lower 90% document length compression ratio')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "_ = plt.subplots(figsize=(10, 6))\n",
    "bins = np.logspace(np.log10(min(turns_per_sum)), np.log10(max(turns_per_sum)))\n",
    "plt.hist(turns_per_sum, color='purple', bins=bins)\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.title('Turns per summary')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.median(turns_per_sum), np.mean(turns_per_sum)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "_ = plt.subplots(figsize=(10, 6))\n",
    "#bins = np.logspace(np.log10(min(chunks_per_file)), np.log10(max(chunks_per_file)))\n",
    "bins = np.linspace(min(chunks_per_file), max(chunks_per_file), 50)\n",
    "plt.hist(chunks_per_file, color='red', bins=bins)\n",
    "#plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.title('Chunks per file')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.median(chunks_per_file), np.mean(chunks_per_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "_ = plt.subplots(figsize=(10, 6))\n",
    "#bins = np.logspace(np.log10(min(turns_per_file)), np.log10(max(turns_per_file)))\n",
    "bins = np.linspace(min(turns_per_file), max(turns_per_file), 50)\n",
    "plt.hist(turns_per_file, color='gold', bins=bins)\n",
    "#plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.title('Turns per file')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.median(turns_per_file), np.mean(turns_per_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "_ = plt.subplots(figsize=(10, 6))\n",
    "#bins = np.logspace(np.log10(min(tokens_per_file)), np.log10(max(tokens_per_file)))\n",
    "bins = np.linspace(min(tokens_per_file), max(tokens_per_file), 50)\n",
    "plt.hist(tokens_per_file, color='aqua', bins=bins)\n",
    "#plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.title('Tokens per file')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.median(tokens_per_file), np.mean(tokens_per_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For estimating the number of turns to take for each summary line in non-annotated data (i.e. C3)\n",
    "print((np.median(turns_per_sum), np.median(tokens_per_file) * np.median(turns_per_sum) / np.median(turns_per_file)))\n",
    "print((np.mean(turns_per_sum), np.mean(tokens_per_file) * np.mean(turns_per_sum) / np.mean(turns_per_file)))\n",
    "np.median(utt_lens), np.mean(utt_lens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(def get_tokens():\n",
    "    # Extract tokens from all the data\n",
    "    tokenizer = dataset.get_tokenizer()\n",
    "    tokens = defaultdict(lambda: 0)\n",
    "    speaker_tokens = defaultdict(lambda: 0)\n",
    "\n",
    "    # CRD3 Data\n",
    "    for speaker_strings, utt_strings, summary_string in tqdm(dataset.iter_chunk(), total=n_chunks):\n",
    "        for token in tokenizer(summary_string.lower()):\n",
    "            tokens[token.text] += 1\n",
    "        for token in (t for s in speaker_strings for t in tokenizer(s.lower())):\n",
    "            speaker_tokens[token.text] += 1\n",
    "        for token in (t for s in utt_strings for t in tokenizer(s.lower())):\n",
    "            tokens[token.text] += 1\n",
    "\n",
    "    # Campaign 3 ep. 1\n",
    "    with open('../data/C3E001.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                speaker_idx = line.index(':')\n",
    "            except Exception as e:\n",
    "                print(line)\n",
    "                raise e\n",
    "            speaker_strings = line[:speaker_idx].lower()\n",
    "            speaker_strings = speaker_strings.replace('and', '')  # Remove 'and' from speakers\n",
    "            utt_strings = line[speaker_idx + 1:].lower()\n",
    "\n",
    "            for token in (t for t in tokenizer(speaker_strings)):\n",
    "                speaker_tokens[token.text] += 1\n",
    "            for token in (t for t in tokenizer(utt_strings)):\n",
    "                tokens[token.text] += 1\n",
    "\n",
    "    # Episode blurbs from the fandom\n",
    "    blurb_df = pd.read_csv('../data/CR_blurbs.tsv', sep='\\t')\n",
    "    for token in (t for s in blurb_df['summary'].values.tolist() for t in tokenizer(s.lower())):\n",
    "            tokens[token.text] += 1\n",
    "\n",
    "    return list(speaker_tokens.keys()), list(tokens.keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spkr_strings, strings = get_tokens()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(spkr_strings)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spkr_strings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(strings)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[s for s in strings if ':' in s]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[s for s in strings if ']' in s or '[' in s]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[s for s in strings if '.' in s]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[s for s in strings if '-' in s]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CRD3_vocab = Vocab(strings=strings)\n",
    "CRD3_spkr_vocab = Vocab(strings=spkr_strings)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(CRD3_vocab), len(CRD3_spkr_vocab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CRD3_vocab.strings[11113032409865315573]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CRD3_spkr_vocab.strings['matt']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hash_idxs = np.array([CRD3_vocab.strings[s] for s in strings])\n",
    "spkr_hash_idxs = np.array([CRD3_spkr_vocab.strings[s] for s in spkr_strings])\n",
    "hash_idxs.shape, spkr_hash_idxs.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.save('../data/CRD3_vocab_hash_idxs.npy', hash_idxs)\n",
    "np.save('../data/CRD3_vocab_spkr_hash_idxs.npy', spkr_hash_idxs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CRD3_vocab.to_disk('../data/CRD3_vocab')\n",
    "CRD3_spkr_vocab.to_disk('../data/CRD3_spkr_vocab')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
